import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import os

def open_files_in_dir(path):
    files = {}
    with os.scandir(path) as entries:
        for entry in entries:
            if entry.is_file():
                files[entry.name.split('.')[0]] = pd.read_csv(entry)
    return files

basepath = 'C:/datasets/sales/'
files_sales = open_files_in_dir(basepath)
print(files_sales.keys())

## DQC class

At first, we can create class which will encapsulate some information about given files.

class DQC:
    
    def __init__(this, our_files):
        this.files = our_files
    
    def print_head(this, table_name):
        print(this.files[table_name].head())
        
    def are_there_any_duplicates(this):
        
        sum_of_dupl = 0
        tables = []
        
        for file in this.files:
            if this.files[file].duplicated().sum()!=0:
                tables.append(file)
                sum_of_dupl += this.files[file].duplicated().sum()
                
            else: sum_of_dupl += this.files[file].duplicated().sum()
                
        return f"The amount of duplicated data in all the tables: {sum_of_dupl}; Tables with duplicates: {tables}"
    
    def are_there_any_null_values(this):
        
        info = []
        
        for file in this.files:
            info.append(f"Table {file} contains {this.files[file].isna().sum().sum()} empty raws")
            
        return info
    
    def are_there_any_outliers(this):
        
        num_columns = {}
        for file in this.files:
            temp_table = this.files[file]
            for col in temp_table.columns:
                if (temp_table[col].dtype != "O") and ('id' not in col and col!='ID'):
                    if (file in num_columns.keys()):
                        num_columns[file].append(col)
                    else:
                        num_columns[file] = [col]
                        
        for file in this.files:
            if file in num_columns.keys():
                plt.figure()
                plt.title(f"Boxplot for {file} table")
                sns.boxplot(data = this.files[file][num_columns[file]])
            else:
                print(f'There are no numeric columns in {file} table')
        
    def columns_type_and_structure(this):
        
        for file in this.files:
            print(f'Table "{file}" with {this.files[file].shape[0]} values:')
            print(this.files[file].dtypes)
            print('\n')
            
    def are_there_any_negative_values(this):
        
        temp_table = this.files['sales_train']
        for col in temp_table.columns:
            if temp_table[col].dtype != "O":
                negs =  len(temp_table[temp_table[col]<0])
                print(f"The precent of negative values in sales_train table in {col} column: {round(negs*100/len(temp_table),5)}")
                if negs!=0: 
                    print('\n')
                    print(temp_table[temp_table[col]<0].head())
                    print('\n')
                    
    def the_returned(this):
        
        sales_train = this.files['sales_train']
        returned = sales_train[sales_train['item_cnt_day']<=0]
        
        return returned
    
    def is_actually_a_return(this):
        
        returned = this.the_returned()
        colors = sns.color_palette('pastel')[0:5]
        
        plt.figure()
        plt.title('Item ids:')
        plt.pie(returned['item_id'].value_counts().head(), labels = returned['item_id'].value_counts().head().index, colors = colors)
        
        plt.figure()
        plt.title('Shop ids:')
        plt.pie(returned['shop_id'].value_counts().head(), labels = returned['shop_id'].value_counts().head().index, colors = colors)
        
        dif_date = returned['date'].nunique()
        print('Amount of unique dates:',dif_date)
        
    def consistancy_info(this):
        
        for file in this.files:
            for col in this.files[file].columns:
                if '_id' in col:
                    print(f'{col} of {file} has {this.files[file][col].nunique()} unique values')
    
    def sales_per_time_visualisation(this):
        pass


## DQC methods

dqc = DQC(files_sales)

dqc.columns_type_and_structure()

6 tables, 2 of them contain test values.

There are 2935849 values in train set and 214200 in test set (about 14:1).

In test values we have amount of sold items in a month while in training set we use daily measure. Date is an object, can be converted in datetime type, then we also can lessen periods of time from date to look at some dynamics.

dqc.consistancy_info()

From amount of unique values we can assume that there are enough information in tables for merging them on ids.

dqc.print_head('sales_train')

From first 5 rows we already see that there are returns (neg item_cnt_day). We can look at it closely.

dqc.are_there_any_negative_values()

Negative values can be deleted.

dqc.is_actually_a_return()

Returns happen with a lot of different type of items in a lot of different shops. The dates are also pretty diverse.

dqc.are_there_any_null_values()

No missing data.

dqc.are_there_any_duplicates()

Found some duplicated rows. Only 6, so it can be safely deleted or ignored.

dqc.are_there_any_outliers()

Explicit outliers in item_price. Should be dropped.


### Merging?

##норм делать это здесь?

train = pd.merge(files_sales['sales_train'], files_sales['shops'], on='shop_id', how = 'left')
train = pd.merge(train, files_sales['items'], on='item_id', how = 'left')
train = pd.merge(train, files_sales['item_categories'], on='item_category_id', how = 'left')
        
train.drop(['shop_id','item_id','item_category_id'],inplace=True, axis=1)
        
test = pd.merge(files_sales['test'], files_sales['shops'], on='shop_id', how = 'left')
test = pd.merge(test, files_sales['items'], on='item_id', how = 'left')
test = pd.merge(test, files_sales['item_categories'], on='item_category_id', how = 'left')
        
test.drop(['shop_id','item_id','item_category_id'],inplace=True, axis=1)

### Categorial feathures

train['shop_name'].value_counts().head(20)

As we can see names have a pattern: location - type - shop name. We can extract some information here, maybe it will be useful.

train['location'] = train['shop_name'].str.split(" ").str[0]
train['shop_type'] = train['shop_name'].str.split(" ").str[1]

train['location'].value_counts()
train['shop_type'].value_counts()

train.loc[train['location']=='Цифровой','location'] = 'Цифровой склад'
train.loc[train['location']=='Сергиев','location'] = 'Сергиев посад'
train.loc[train['shop_type']=='Орджоникидзе,','shop_type'] = 'Не указан'
train.loc[train['shop_type']=='Посад','shop_type'] = 'ТЦ'
train.loc[train['shop_type']=='"Распродажа"','shop_type'] = 'Не указан'
train.loc[train['shop_type']=='(Плехановская,','shop_type'] = 'Не указан'
train.loc[train['shop_type']=='склад','shop_type'] = 'Не указан'
train.loc[train['shop_type']=='ул.','shop_type'] = 'Не указан'
train.loc[train['shop_type']=='Торговля','shop_type'] = 'Не указан'

train['shop_type'].value_counts()

train.head()

Later categorical values should be changed to numerical.

top_10_locs = train['location'].value_counts().head(10)
top_10_locs = top_10_locs.to_frame().reset_index()
top_10_locs.columns = ['location', 'count']
sns.barplot(data=top_10_locs, x='count', y='location', palette = 'pastel')
plt.title('Top 10 most popular locations')
plt.ylabel('Place')
plt.xlabel('Count')
plt.show()

## ETL

import sys
module_path = "C:\\github\\\internship"
if module_path not in sys.path:
    sys.path.append(module_path)
    
import ETL as etl

etl_train = etl.ETL(train)

'''
class ETL:
    
    def __init__(this, data):
        
        this.data = data
    
    def length(this):
        
        data_len = this.data.shape[0]
        
        print(f"The amount of rows in data set: {data_len}")
        print('The amount of null values in train:',this.data.isna().sum().sum())
        
    def delete_neg_in_price_and_sales(this):
        
        this.data = this.data[this.data['item_price']>0]
        this.data = this.data[this.data['item_cnt_day']>0]
        
        return this.data
    
    def delete_duplicates(this):
        
        return this.data.drop_duplicates()
    
    def remove_outliers(this, col): ##

        Q1 = this.data[col].quantile(0.10)
        Q3 = this.data[col].quantile(0.90)
        IQR = Q3 - Q1
        this.data = this.data[~((this.data[col] < (Q1 - 1.5 * IQR)) | (this.data[col] > (Q3 + 1.5 * IQR)))]
        
        return this.data
    
    def date_format(this, col):
        
        this.data[col] = pd.to_datetime(this.data[col], format='%d.%m.%Y')
        
        this.data['year'] = this.data[col].dt.year
        this.data['month'] = this.data[col].dt.month
        this.data['day'] = this.data[col].dt.day
        
        return this.data
    
    ###Only after date converting
    ## через get_dummies слишком большие размерности выходят, какой-то другой метод брать или оставлять пока?
    def cat_to_num(this, col):
        cat = [col for col in this.data.columns if this.data[col].dtype=='O']
        this.data
        return pd.get_dummies(this.data, columns = col)
'''
