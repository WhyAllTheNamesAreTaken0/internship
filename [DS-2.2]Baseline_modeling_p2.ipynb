{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9936ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b9b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('C:/datasets/train_sales.csv')\n",
    "test = pd.read_csv('C:/datasets/test_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05b7ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train.groupby(['date_block_num', 'location','shop_type','shop_id','item_category_id','item_category', 'subcat','item_id',]).agg({'item_cnt_day':'sum','item_price':'mean'}).reset_index()\n",
    "train_ds.columns = ['date_block_num', 'location','shop_type','shop_id','item_category_id','item_category','subcat','item_id','item_cnt_monthly','mean_item_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205db719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_data(data):\n",
    "    \n",
    "    #shop\n",
    "    data[\"loc_cd\"] = LabelEncoder().fit_transform(data[\"location\"])\n",
    "    data[\"shop_type_cd\"] = LabelEncoder().fit_transform(data[\"shop_type\"])\n",
    "    data.drop([\"location\"], axis=1, inplace=True)\n",
    "    data.drop([\"shop_type\"], axis=1, inplace=True)\n",
    "\n",
    "    #item\n",
    "    data[\"item_cat_cd\"] = LabelEncoder().fit_transform(data[\"item_category\"])\n",
    "    data[\"subcat_cd\"] = LabelEncoder().fit_transform(data[\"subcat\"])\n",
    "    data.drop([\"item_category\"], axis=1, inplace=True)\n",
    "    data.drop([\"subcat\"], axis=1, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def one_hot_encode_data(data):\n",
    "    \n",
    "    data = pd.get_dummies(data, columns = ['location', 'shop_type','item_category','subcat'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def mean_encode_data(data):\n",
    "    Mean_encoded_loc = data.groupby(['location'])['item_cnt_monthly'].mean().to_dict()\n",
    "    data['loc_cd'] =  data['location'].map(Mean_encoded_loc)\n",
    "    Mean_encoded_shop = data.groupby(['shop_type'])['item_cnt_monthly'].mean().to_dict()\n",
    "    data['shop_type_cd'] =  data['shop_type'].map(Mean_encoded_shop)\n",
    "    data.drop([\"location\"], axis=1, inplace=True)\n",
    "    data.drop([\"shop_type\"], axis=1, inplace=True)\n",
    "\n",
    "    #item\n",
    "    Mean_encoded_cat= data.groupby(['item_category'])['item_cnt_monthly'].mean().to_dict()\n",
    "    data['item_cat_cd'] =  data['item_category'].map(Mean_encoded_cat)\n",
    "    Mean_encoded_subcat = data.groupby(['subcat'])['item_cnt_monthly'].mean().to_dict()\n",
    "    data['subcat_cd'] =  data['subcat'].map(Mean_encoded_subcat)\n",
    "    data.drop([\"item_category\"], axis=1, inplace=True)\n",
    "    data.drop([\"subcat\"], axis=1, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def novelty_feature(data):\n",
    "    \n",
    "    #novelty\n",
    "    min_values = data.groupby(\"item_id\")[\"date_block_num\"].min().reset_index()\n",
    "    min_values.columns = ['item_id','first_sales_date_block']\n",
    "    data = pd.merge(data, min_values, on='item_id', how = 'left')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def lag_features(df, lags, col_list):\n",
    "    \n",
    "    for col_name in col_list:\n",
    "        tmp = df[[\"date_block_num\", \"shop_id\", \"item_id\", col_name]]\n",
    "        for i in lags:\n",
    "            shifted = tmp.copy()\n",
    "            shifted.columns = [\n",
    "                \"date_block_num\",\n",
    "                \"shop_id\",\n",
    "                \"item_id\",\n",
    "                col_name + \"_lag_\" + str(i),\n",
    "            ]\n",
    "            shifted[\"date_block_num\"] += i\n",
    "            df = pd.merge(\n",
    "                df, shifted, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\"\n",
    "            )\n",
    "    return df\n",
    "\n",
    "def last_halfyear_feathure(train_ds):\n",
    "#last 6 month average of sales\n",
    "    train_ds[\"last_6month_cnt\"] = train_ds[[\"item_cnt_monthly_lag_1\", \"item_cnt_monthly_lag_2\", \"item_cnt_monthly_lag_3\", \"item_cnt_monthly_lag_4\", \"item_cnt_monthly_lag_5\",\"item_cnt_monthly_lag_6\"]].mean(skipna=True, axis=1)\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91670c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def primary_data_for_modeling(data, enc):\n",
    "    \n",
    "    train_ds = novelty_feature(data)\n",
    "    if enc=='OHE':\n",
    "        train_ds = one_hot_encode_data(train_ds)\n",
    "    if enc=='mean':\n",
    "        train_ds = mean_encode_data(train_ds)\n",
    "    if enc=='label':\n",
    "        train_ds = label_encode_data(train_ds)\n",
    "    train_ds = lag_features(train_ds, [1, 2, 3, 4, 5, 6, 12], [\"item_cnt_monthly\"])\n",
    "    train_ds = last_halfyear_feathure(train_ds)\n",
    "    train_ds.fillna(0, inplace=True)\n",
    "    \n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1760d83",
   "metadata": {},
   "source": [
    "Here i changed cross-validation function for it to fit as a parameter of cross-validation of grid search.\n",
    "\n",
    "The whole structure of it is the same except the output is yield indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e0d5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_for_grid(data, month):\n",
    "    \n",
    "    if month>=data.date_block_num.max():\n",
    "        print(\"Cannot be splited\")\n",
    "        \n",
    "    elif month<=data.date_block_num.min():\n",
    "        print(\"Cannot be splited\")\n",
    "    else:\n",
    "        while True:\n",
    "            \n",
    "            ##feature gen.\n",
    "            data_1 = data[:(data[data.date_block_num==(month+1)][-1:].index[0]+1)]\n",
    "            #data_1 = primary_data_for_modeling(data_1, enc)\n",
    "            \n",
    "            train_ds = np.array(data_1[:(data_1[data_1.date_block_num==month][-1:].index[0]+1)].index)\n",
    "            if (month+1)<=data_1.date_block_num.max():\n",
    "                test_ds = np.array(data_1[(data_1[data_1.date_block_num==month][-1:].index[0]+1):(data_1[data_1.date_block_num==(month+1)][-1:].index[0]+1)].index)\n",
    "            else:\n",
    "                test_ds = np.array(data_1[(data_1[data_1.date_block_num==month][-1:].index[0]+1):].index)\n",
    "            \n",
    "            yield train_ds, test_ds\n",
    "\n",
    "            month+=1\n",
    "            \n",
    "            if month>=data.date_block_num.max():\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e3b9a",
   "metadata": {},
   "source": [
    "Next there is a function that do the hyperparameters searching since in the task it is said to experiment with models(?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a9c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_func(train_ds, model, param_space):\n",
    "    \n",
    "            from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "            tscv = cross_validation_for_grid(train_ds, 28)\n",
    "            #train_ds = primary_data_for_modeling(train_ds, enc)\n",
    "            \n",
    "            best_params_models = {}\n",
    "            \n",
    "            grid_search = GridSearchCV(model, param_space, cv=cross_validation_for_grid(train_ds, 30), return_train_score=True, verbose = 5, n_jobs=-1)\n",
    "            grid_search.fit(train_ds.drop('item_cnt_monthly', axis=1), train_ds['item_cnt_monthly'])\n",
    "                \n",
    "            print(model)\n",
    "            print(\"Best params: \", grid_search.best_params_)\n",
    "            print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce8184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what was done here is for the sake of speed\n",
    "train_ds_pred = train_ds[train_ds.date_block_num>11]\n",
    "train_ds_pred = primary_data_for_modeling(train_ds_pred, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32b5dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, test, train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a28343",
   "metadata": {},
   "source": [
    "**Decision Tree:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b6666cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "DecisionTreeRegressor()\n",
      "Best params:  {'max_depth': 3}\n",
      "Best score:  0.41414438632067885\n"
     ]
    }
   ],
   "source": [
    "param_DT = {'max_depth': [3, 5, 6, 8]}\n",
    "grid_search_func(train_ds_pred, DecisionTreeRegressor(), param_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6fe129",
   "metadata": {},
   "source": [
    "**Random Forest:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52032df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomForestRegressor()\n",
      "Best params:  {'max_depth': 5, 'n_estimators': 50, 'n_jobs': -1}\n",
      "Best score:  0.44769956396558985\n"
     ]
    }
   ],
   "source": [
    "param_RF = {'max_depth': [3, 5, 6, 8],\n",
    "           'n_estimators': [50, 100],\n",
    "           'n_jobs':[-1]}\n",
    "\n",
    "grid_search_func(train_ds_pred, RandomForestRegressor(), param_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09808d",
   "metadata": {},
   "source": [
    "**SVR:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0fa719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cell run in this one didn't complete for more than 8 hours, so i didn't ran it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2f2a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def scaler(train_ds):\n",
    "    \n",
    "    # the scaler object (model)\n",
    "    scaler = StandardScaler()\n",
    "    # fit and transform the data\n",
    "    numeric = ['shop_id', 'item_category_id', 'mean_item_price','item_id','shop_id','loc_cd','shop_type_cd','item_cat_cd','subcat_cd',]\n",
    "    scaler.fit(train_ds[numeric])\n",
    "    train_ds[numeric] = scaler.transform(train_ds[numeric])\n",
    "    \n",
    "    return train_ds\n",
    "\n",
    "train_ds_pred = scaler(train_ds_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52125a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    }
   ],
   "source": [
    "param_SVR = {'C': [0.1, 1, 10],\n",
    "             'kernel': ['rbf', 'sigmoid']}\n",
    "\n",
    "grid_search_func(train_ds_pred, SVR(cache_size=7000), param_SVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17343b",
   "metadata": {},
   "source": [
    "**XGBoost:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecdad752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "Best params:  {'max_depth': 8, 'n_estimators': 50, 'n_jobs': -1}\n",
      "Best score:  0.4247756435743307\n"
     ]
    }
   ],
   "source": [
    "param_XGB = {'max_depth': [3, 5, 6, 8],\n",
    "             'n_estimators': [50, 100],\n",
    "             'n_jobs':[-1]}\n",
    "grid_search_func(train_ds_pred, xgb.XGBRegressor(), param_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec6f8b3",
   "metadata": {},
   "source": [
    "**LGBM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78b489eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "LGBMRegressor()\n",
      "Best params:  {'max_depth': 3, 'n_estimators': 50, 'n_jobs': -1}\n",
      "Best score:  0.42349810969927154\n"
     ]
    }
   ],
   "source": [
    "param_LGB = {'max_depth': [3, 5, 6, 8],\n",
    "             'n_estimators': [50, 100],\n",
    "             'n_jobs':[-1]}\n",
    "grid_search_func(train_ds_pred, lgb.LGBMRegressor(), param_LGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c1c21",
   "metadata": {},
   "source": [
    "There lower score is in XGB and LGBM Regressors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
